{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f10260c7-85ba-45eb-96f7-80e7af901802",
   "metadata": {},
   "source": [
    "# Feature Engineering & Representation Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dfadd0-a7d6-4092-9148-cc6c3dee8ee6",
   "metadata": {},
   "source": [
    "In this notebook we will implement a comprehensive feature engineering pipeline that combines three types of features: TF-IDF for capturing important terms, Word2Vec for semantic relationships, and metadata features from review statistics. This approach will help us ensure that we capture different aspects of the reviews for better representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6a5040-f4f7-47fe-8347-03ed7bf3dab2",
   "metadata": {},
   "source": [
    "### 1. Initial Setup and Data Loading:\n",
    "#### Import Libraries\n",
    "We will begin by importing necessary libraries for text vectorization (TF-IDF), word embeddings (Word2Vec), and other preprocessing tools. The cleaned dataset from our previous preprocessing steps will serve as our starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90df0d9e-2dd4-4260-9d06-85804a47555f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5d834a-dde8-4ec3-b25a-692093b6611c",
   "metadata": {},
   "source": [
    "#### Load Dataset\n",
    "Next, we are going to load the cleaned dataset and examine its structure such as shape, column names, data types and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea5c3e19-4186-4ab3-9afc-e98183af109d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>reviews</th>\n",
       "      <th>review_count</th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>sentiment_details</th>\n",
       "      <th>total_helpful_votes</th>\n",
       "      <th>preprocessed_reviews</th>\n",
       "      <th>preprocessed_tokens</th>\n",
       "      <th>comb_preprocessed_reviews</th>\n",
       "      <th>comb_preprocessed_tokens</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2734888454</td>\n",
       "      <td>['My dogs loves this chicken but its a product...</td>\n",
       "      <td>2</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>{'positive': 1, 'neutral': 0, 'negative': 1}</td>\n",
       "      <td>1</td>\n",
       "      <td>['my dogs loves this chicken but its a product...</td>\n",
       "      <td>['dog love chicken product china buying anymor...</td>\n",
       "      <td>my dogs loves this chicken but its a product f...</td>\n",
       "      <td>dog love chicken product china buying anymore ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7800648702</td>\n",
       "      <td>[\"This came in a HUGE tin, much bigger than I ...</td>\n",
       "      <td>2</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>{'positive': 1, 'neutral': 1, 'negative': 0}</td>\n",
       "      <td>0</td>\n",
       "      <td>['this came in a huge tin much bigger than i e...</td>\n",
       "      <td>['came huge tin much bigger expected cooky swe...</td>\n",
       "      <td>this came in a huge tin much bigger than i exp...</td>\n",
       "      <td>came huge tin much bigger expected cooky sweet...</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00002NCJC</td>\n",
       "      <td>['Why is this $[...] when the same product is ...</td>\n",
       "      <td>2</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>{'positive': 2, 'neutral': 0, 'negative': 0}</td>\n",
       "      <td>0</td>\n",
       "      <td>['why is this when the same product is availab...</td>\n",
       "      <td>['product available http www amazon com victor...</td>\n",
       "      <td>why is this when the same product is available...</td>\n",
       "      <td>product available http www amazon com victor f...</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B00002Z754</td>\n",
       "      <td>[\"I just received my shipment and could hardly...</td>\n",
       "      <td>2</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>{'positive': 2, 'neutral': 0, 'negative': 0}</td>\n",
       "      <td>17</td>\n",
       "      <td>['i just received my shipment and could hardly...</td>\n",
       "      <td>['received shipment could hardly wait try prod...</td>\n",
       "      <td>i just received my shipment and could hardly w...</td>\n",
       "      <td>received shipment could hardly wait try produc...</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B00004RAMV</td>\n",
       "      <td>[\"Large, therefore keeps its content for a whi...</td>\n",
       "      <td>9</td>\n",
       "      <td>2.111111</td>\n",
       "      <td>{'positive': 3, 'neutral': 0, 'negative': 6}</td>\n",
       "      <td>17</td>\n",
       "      <td>['large therefore keeps its content for a whil...</td>\n",
       "      <td>['large therefore keep content easy fill use r...</td>\n",
       "      <td>large therefore keeps its content for a while ...</td>\n",
       "      <td>large therefore keep content easy fill use reu...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id                                            reviews  \\\n",
       "0  2734888454  ['My dogs loves this chicken but its a product...   \n",
       "1  7800648702  [\"This came in a HUGE tin, much bigger than I ...   \n",
       "2  B00002NCJC  ['Why is this $[...] when the same product is ...   \n",
       "3  B00002Z754  [\"I just received my shipment and could hardly...   \n",
       "4  B00004RAMV  [\"Large, therefore keeps its content for a whi...   \n",
       "\n",
       "   review_count  avg_rating                             sentiment_details  \\\n",
       "0             2    3.500000  {'positive': 1, 'neutral': 0, 'negative': 1}   \n",
       "1             2    4.000000  {'positive': 1, 'neutral': 1, 'negative': 0}   \n",
       "2             2    4.500000  {'positive': 2, 'neutral': 0, 'negative': 0}   \n",
       "3             2    5.000000  {'positive': 2, 'neutral': 0, 'negative': 0}   \n",
       "4             9    2.111111  {'positive': 3, 'neutral': 0, 'negative': 6}   \n",
       "\n",
       "   total_helpful_votes                               preprocessed_reviews  \\\n",
       "0                    1  ['my dogs loves this chicken but its a product...   \n",
       "1                    0  ['this came in a huge tin much bigger than i e...   \n",
       "2                    0  ['why is this when the same product is availab...   \n",
       "3                   17  ['i just received my shipment and could hardly...   \n",
       "4                   17  ['large therefore keeps its content for a whil...   \n",
       "\n",
       "                                 preprocessed_tokens  \\\n",
       "0  ['dog love chicken product china buying anymor...   \n",
       "1  ['came huge tin much bigger expected cooky swe...   \n",
       "2  ['product available http www amazon com victor...   \n",
       "3  ['received shipment could hardly wait try prod...   \n",
       "4  ['large therefore keep content easy fill use r...   \n",
       "\n",
       "                           comb_preprocessed_reviews  \\\n",
       "0  my dogs loves this chicken but its a product f...   \n",
       "1  this came in a huge tin much bigger than i exp...   \n",
       "2  why is this when the same product is available...   \n",
       "3  i just received my shipment and could hardly w...   \n",
       "4  large therefore keeps its content for a while ...   \n",
       "\n",
       "                            comb_preprocessed_tokens sentiment  \\\n",
       "0  dog love chicken product china buying anymore ...   neutral   \n",
       "1  came huge tin much bigger expected cooky sweet...  positive   \n",
       "2  product available http www amazon com victor f...  positive   \n",
       "3  received shipment could hardly wait try produc...  positive   \n",
       "4  large therefore keep content easy fill use reu...   neutral   \n",
       "\n",
       "   sentiment_encoded  \n",
       "0                  1  \n",
       "1                  2  \n",
       "2                  2  \n",
       "3                  2  \n",
       "4                  1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the cleaned dataset\n",
    "filtered_reviews = pd.read_csv('cleaned_reviews.csv')\n",
    "filtered_reviews.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "677f92f2-3a42-4e85-b7bb-98f31a68b546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40289 entries, 0 to 40288\n",
      "Data columns (total 12 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   product_id                 40289 non-null  object \n",
      " 1   reviews                    40289 non-null  object \n",
      " 2   review_count               40289 non-null  int64  \n",
      " 3   avg_rating                 40289 non-null  float64\n",
      " 4   sentiment_details          40289 non-null  object \n",
      " 5   total_helpful_votes        40289 non-null  int64  \n",
      " 6   preprocessed_reviews       40289 non-null  object \n",
      " 7   preprocessed_tokens        40289 non-null  object \n",
      " 8   comb_preprocessed_reviews  40289 non-null  object \n",
      " 9   comb_preprocessed_tokens   40289 non-null  object \n",
      " 10  sentiment                  40289 non-null  object \n",
      " 11  sentiment_encoded          40289 non-null  int64  \n",
      "dtypes: float64(1), int64(3), object(8)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "filtered_reviews.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb8029f-b7a7-4816-8b14-a6209fcad8fb",
   "metadata": {},
   "source": [
    "##### Observations:\n",
    "- Successfully loaded dataset with all preprocessing columns intact\n",
    "- Dataset maintains its structure with 10 columns and proper data types\n",
    "- No missing values in any columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181bbd40-5681-4b39-866f-4d8957a8be01",
   "metadata": {},
   "source": [
    "### 2. Data Preparation for Feature Engineering:\n",
    "We will organize our features and target variables for the train-test split, selecting relevant columns for different feature types (text-based and metadata)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2aa4f275-94df-4aa5-ad9d-36e2db57efe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_split(filtered_reviews):\n",
    "    \"\"\"\n",
    "    Prepare X and y before train-test split.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    filtered_reviews : pandas.DataFrame\n",
    "        Preprocessed review data\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X : pandas.DataFrame\n",
    "        Features to be used for splitting (text and metadata)\n",
    "    y : numpy.ndarray\n",
    "        Target variable (sentiment)\n",
    "    \"\"\"\n",
    "    # Select relevant columns for X\n",
    "    X = filtered_reviews[[\n",
    "        'comb_preprocessed_reviews',  # for TF-IDF\n",
    "        'comb_preprocessed_tokens',   # for Word2Vec\n",
    "        'review_count',              # metadata\n",
    "        'avg_rating',                 # metadata\n",
    "        'total_helpful_votes'\n",
    "    ]].copy()\n",
    "    \n",
    "    # Target variable\n",
    "    y = filtered_reviews['sentiment_encoded'].values\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854afb6c-48d2-4e3b-a698-fd7c2577e031",
   "metadata": {},
   "source": [
    "This above function will help create clear separation between features (X) and target variable (y). Selected balanced mix of text and metadata features and maintained both raw preprocessed reviews and tokenized versions for different feature extraction methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f4c2d52-ae2c-42ce-9d22-b06a70239f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_train_test_split(X, y, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Perform train-test split on the data.\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                        test_size=test_size,\n",
    "                                                        random_state=random_state,\n",
    "                                                        stratify=y)\n",
    "    \n",
    "    print(\"Data split sizes:\")\n",
    "    print(f\"Training set: {len(X_train)} samples\")\n",
    "    print(f\"Test set: {len(X_test)} samples\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aab16c0-12a4-47c8-94e8-3f32a2137e39",
   "metadata": {},
   "source": [
    "This will help split the data into training (80%) and test (20%) sets using stratification method to maintain balanced sentiment distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f859c5-6852-4bc9-9dcb-0f170596a53e",
   "metadata": {},
   "source": [
    "### 3. Feature Creation:\n",
    "Next we will implement three types of features to capture different aspects of the reviews:\n",
    "\n",
    "1. TF-IDF features for capturing important terms (5000 features)\n",
    "2. Word2Vec embeddings for semantic relationships (100 features)\n",
    "3. Metadata features including review count, average rating and total_helpful_votes (3 features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58166c47-819f-4af4-bf39-745ee7779dd1",
   "metadata": {},
   "source": [
    "This function will preprocess the text to TF-IDF (Term Frequency-Inverse Document Frequency) as feature representations using TfidfVectorizer. TF-IDF converts the text into numerical feature vectors and captures the importance of words compared to the entire collection. It will help generate 5000 features capturing important terms and bigrams using L2 normalization to handle varying review lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41759e8c-53fa-41bf-b943-adc2933e4e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfidf_features(X_train, X_test, max_features=5000):\n",
    "    \"\"\"\n",
    "    Create TF-IDF features for train and test sets.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : pandas.DataFrame\n",
    "        Training data\n",
    "    X_test : pandas.DataFrame\n",
    "        Test data\n",
    "    max_features : int\n",
    "        Maximum number of TF-IDF features\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict containing:\n",
    "        - X_train_tfidf: TF-IDF features for training data\n",
    "        - X_test_tfidf: TF-IDF features for test data\n",
    "        - tfidf: Fitted TfidfVectorizer\n",
    "    \"\"\"\n",
    "    print(\"\\nCreating TF-IDF features...\")\n",
    "    tfidf = TfidfVectorizer(\n",
    "        max_features=max_features,\n",
    "        ngram_range=(1, 2),\n",
    "        strip_accents='unicode',\n",
    "        analyzer='word',\n",
    "        token_pattern=r'\\w{1,}',\n",
    "        stop_words='english',\n",
    "        norm='l2')\n",
    "    \n",
    "    X_train_tfidf = tfidf.fit_transform(X_train['comb_preprocessed_reviews'])\n",
    "    X_test_tfidf = tfidf.transform(X_test['comb_preprocessed_reviews'])\n",
    "    \n",
    "    print(f\"TF-IDF features shape: {X_train_tfidf.shape[1]} features\")\n",
    "    \n",
    "    return {'X_train_tfidf': X_train_tfidf,\n",
    "            'X_test_tfidf': X_test_tfidf,\n",
    "            'tfidf': tfidf}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753a8d34-d1bd-44f1-b06c-e194818d7c70",
   "metadata": {},
   "source": [
    "Next we will implement Word2Vec embeddings to capture semantic relationships between words in the reviews. Word2Vec learns distributed vector representations of words based on their context, where words appearing in similar contexts are mapped to nearby points in the vector space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49489c5e-6f93-4dbc-a3d8-3313670c370d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_document_vectors(text_series, w2v_model):\n",
    "    \"\"\"\n",
    "    Create document vectors from text using Word2Vec model.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text_series : pandas.Series\n",
    "        Series containing tokenized text\n",
    "    w2v_model : Word2Vec\n",
    "        Trained Word2Vec model\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        Document vectors\n",
    "    \"\"\"\n",
    "    doc_vectors = np.zeros((len(text_series), w2v_model.vector_size))\n",
    "    \n",
    "    for idx, text in enumerate(text_series):\n",
    "        tokens = text.split()\n",
    "        valid_tokens = [token for token in tokens if token in w2v_model.wv]\n",
    "        if valid_tokens:\n",
    "            doc_vectors[idx] = np.mean([w2v_model.wv[token] for token in valid_tokens], axis=0)\n",
    "            \n",
    "    return doc_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cc60d8-4fb6-469b-8b91-2afc18bee13d",
   "metadata": {},
   "source": [
    "We will train the model only on the training data to prevent data leakage, use a context window of 5 words and minimum word frequency of 2. Each word will be represented by a 100-dimensional vector, and document vectors are created by averaging the vectors of all words in a review. This approach will help us capture semantic similarities and relationships between reviews that might not be captured by simple term frequency approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5579caac-9851-4bda-8c23-e5ddc48f90e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word2vec_features(X_train, X_test, vector_size=100):\n",
    "    \"\"\"\n",
    "    Create Word2Vec features for train and test sets.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : pandas.DataFrame\n",
    "        Training data\n",
    "    X_test : pandas.DataFrame\n",
    "        Test data\n",
    "    vector_size : int\n",
    "        Size of word vectors\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict containing:\n",
    "        - X_train_w2v: Word2Vec features for training data\n",
    "        - X_test_w2v: Word2Vec features for test data\n",
    "        - w2v_model: Trained Word2Vec model\n",
    "    \"\"\"\n",
    "    print(\"\\nCreating Word2Vec features...\")\n",
    "    \n",
    "    # Train Word2Vec on training data only\n",
    "    train_tokens = [text.split() for text in X_train['comb_preprocessed_tokens']]\n",
    "    w2v_model = Word2Vec(sentences=train_tokens,\n",
    "                         vector_size=vector_size,\n",
    "                         window=5,\n",
    "                         min_count=2,\n",
    "                         workers=4)\n",
    "    \n",
    "    # Create document vectors for both sets\n",
    "    X_train_w2v = create_document_vectors(X_train['comb_preprocessed_tokens'], w2v_model)\n",
    "    X_test_w2v = create_document_vectors(X_test['comb_preprocessed_tokens'], w2v_model)\n",
    "    \n",
    "    print(f\"Word2Vec features shape: {X_train_w2v.shape[1]} features\")\n",
    "    \n",
    "    return {'X_train_w2v': X_train_w2v,\n",
    "            'X_test_w2v': X_test_w2v,\n",
    "            'w2v_model': w2v_model}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0796f6-1e33-4846-952f-6ec87949f328",
   "metadata": {},
   "source": [
    "In this function, incorporated 3 statistical features about reviews that provide context beyond text content and maintained values as numeric without encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ae3e235-afe5-4ba1-8ccc-ddd234aa37a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metadata_features(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Create metadata features for train and test sets.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : pandas.DataFrame\n",
    "        Training data\n",
    "    X_test : pandas.DataFrame\n",
    "        Test data\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict containing:\n",
    "        - X_train_meta: Metadata features for training data\n",
    "        - X_test_meta: Metadata features for test data\n",
    "    \"\"\"\n",
    "    print(\"\\nCreating metadata features...\")\n",
    "    metadata_columns = ['review_count', 'avg_rating', 'total_helpful_votes']\n",
    "    \n",
    "    X_train_meta = X_train[metadata_columns].values\n",
    "    X_test_meta = X_test[metadata_columns].values\n",
    "    \n",
    "    print(f\"Metadata features shape: {X_train_meta.shape[1]} features\")\n",
    "    \n",
    "    return {'X_train_meta': X_train_meta,\n",
    "            'X_test_meta': X_test_meta}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a02619-722b-4ed8-a732-fd1249888cd5",
   "metadata": {},
   "source": [
    "### 4. Feature Engineering Pipeline:\n",
    "Lastly we will run the entire feature engineering pipeline to combine all features into a unified representation and apply standardization to ensure all features are on the same scale for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ed45423-820e-4b66-9546-2fca50f66df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_feature_engineering_pipeline(filtered_reviews, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Run the complete feature engineering pipeline including standardization.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    filtered_reviews : pandas.DataFrame\n",
    "        Preprocessed review data\n",
    "    test_size : float, default=0.2\n",
    "        Size of test set\n",
    "    random_state : int, default=42\n",
    "        Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X_train : numpy.ndarray\n",
    "        Training features\n",
    "    X_test : numpy.ndarray\n",
    "        Test features\n",
    "    y_train : numpy.ndarray\n",
    "        Training labels\n",
    "    y_test : numpy.ndarray\n",
    "        Test labels\n",
    "    transformers : dict\n",
    "        Dictionary containing fitted transformers\n",
    "    \"\"\"\n",
    "    # 1. Prepare data for splitting\n",
    "    print(\"Preparing data for split...\")\n",
    "    X, y = prepare_data_for_split(filtered_reviews)\n",
    "    \n",
    "    # 2. Perform train-test split\n",
    "    print(\"\\nPerforming train-test split...\")\n",
    "    X_train, X_test, y_train, y_test = perform_train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # 3. Create each type of features\n",
    "    tfidf_features = create_tfidf_features(X_train, X_test)\n",
    "    w2v_features = create_word2vec_features(X_train, X_test)\n",
    "    meta_features = create_metadata_features(X_train, X_test)\n",
    "\n",
    "    # 4. Combine features\n",
    "    # Combine training features\n",
    "    X_train_combined = np.hstack([tfidf_features['X_train_tfidf'].toarray(),\n",
    "                                  w2v_features['X_train_w2v'],\n",
    "                                  meta_features['X_train_meta']])\n",
    "    \n",
    "    # Combine test features\n",
    "    X_test_combined = np.hstack([tfidf_features['X_test_tfidf'].toarray(),\n",
    "                                 w2v_features['X_test_w2v'],\n",
    "                                 meta_features['X_test_meta']])\n",
    "    \n",
    "    print(f\"\\nFinal feature shapes:\")\n",
    "    print(f\"Training set: {X_train_combined.shape}\")\n",
    "    print(f\"Test set: {X_test_combined.shape}\")\n",
    "\n",
    "    # 5. Standardize all features\n",
    "    print(\"\\nStandardizing all features...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_combined)\n",
    "    X_test_scaled = scaler.transform(X_test_combined)\n",
    "    \n",
    "    # Collect transformers\n",
    "    transformers = {'tfidf': tfidf_features['tfidf'],\n",
    "                    'w2v': w2v_features['w2v_model'],\n",
    "                    'scaler': scaler}\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test, transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcb4275-7b15-4789-92fa-00aff423658d",
   "metadata": {},
   "source": [
    "### 5. Final Pipeline Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6dfef0f-636e-4256-916e-170c46f8b980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for split...\n",
      "\n",
      "Performing train-test split...\n",
      "Data split sizes:\n",
      "Training set: 32231 samples\n",
      "Test set: 8058 samples\n",
      "\n",
      "Creating TF-IDF features...\n",
      "TF-IDF features shape: 5000 features\n",
      "\n",
      "Creating Word2Vec features...\n",
      "Word2Vec features shape: 100 features\n",
      "\n",
      "Creating metadata features...\n",
      "Metadata features shape: 3 features\n",
      "\n",
      "Final feature shapes:\n",
      "Training set: (32231, 5103)\n",
      "Test set: (8058, 5103)\n",
      "\n",
      "Standardizing all features...\n",
      "\n",
      "Final dataset shapes:\n",
      "X_train: (32231, 5103)\n",
      "X_test: (8058, 5103)\n",
      "y_train: (32231,)\n",
      "y_test: (8058,)\n"
     ]
    }
   ],
   "source": [
    "# Run the complete pipeline\n",
    "X_train, X_test, y_train, y_test, transformers = run_feature_engineering_pipeline(\n",
    "    filtered_reviews,\n",
    "    test_size=0.2)\n",
    "\n",
    "# Print final shapes\n",
    "print(\"\\nFinal dataset shapes:\")\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "print(f\"y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7691640-801d-4346-8ca4-6cbf240f623a",
   "metadata": {},
   "source": [
    "### Final Observations:\n",
    "\n",
    "1. Feature Engineering Results:\n",
    "    - Total features created: 5,103 (5000 TF-IDF + 100 Word2Vec + 3 metadata)\n",
    "    - Training set: 32,231 samples\n",
    "    - Test set: 8,058 samples\n",
    "    \n",
    "\n",
    "2. Feature Engineering Pipeline Success:\n",
    "\n",
    "    - Successfully combined multiple feature types and standardized\n",
    "    - No missing values in final feature matrices\n",
    "    - Preserved train/test separation throughout\n",
    "\n",
    "3. Next Steps:\n",
    "\n",
    "    - Features ready for model training\n",
    "    - Saved transformers for future inference\n",
    "    - Data saved in compressed numpy format for efficient loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a0dc171-99c2-45bb-ad1b-426f1e4fb028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save numpy arrays\n",
    "np.savez('data.npz', X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48f23a34-8afe-4fde-a5bc-ae466e2a9275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['transformers.joblib']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save transformers\n",
    "joblib.dump(transformers, 'transformers.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03b66f6-75cd-426c-ab04-806fc941d70f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
